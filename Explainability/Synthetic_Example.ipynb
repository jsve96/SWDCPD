{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(d,N,seed,ndrift):\n",
    "    np.random.seed(seed)\n",
    "    #### generate random mean\n",
    "    mu1 = np.random.randn(d)\n",
    "    #sample random indicies\n",
    "    ind = list(np.random.choice(np.arange(0,d),ndrift,replace=False))\n",
    "    severity = np.random.normal(2,1,ndrift)\n",
    "    print(severity)\n",
    "    mu2 = mu1.copy()\n",
    "    mu2[ind] = mu2[ind] + severity\n",
    "\n",
    "    Sigma = np.eye(d)\n",
    "    Sigma_y = Sigma.copy()\n",
    "    Sigma_y[0,0] = 1\n",
    "    X = np.random.multivariate_normal(mu1,Sigma,size=N)\n",
    "    Y = np.random.multivariate_normal(mu2,Sigma_y,size=N)\n",
    "    return ind,severity, X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drifted_ind, severity, X,Y = generateData(50,1000,0,10)\n",
    "# beta_overall = []\n",
    "# SWDs_overall = []\n",
    "# fig,ax = plt.subplots(2,1,figsize=(12,8))\n",
    "# for _ in range(10):\n",
    "#     drifted_ind, severity, X,Y = generateData(20,1000,_,10)\n",
    "#     removed, betas,SWDs,_ = remove_important_features_syn(X,Y,15,N_Theta=1000)\n",
    "#     beta_overall.append(betas)\n",
    "#     SWDs_overall.append(SWDs)\n",
    "#     ax[0].plot(range(len(betas)),betas,marker='.',color='grey',alpha=0.4)\n",
    "#     ax[1].plot(range(len(SWDs)),SWDs,marker='.',color='grey',alpha=0.4)\n",
    "\n",
    "# ax[0].plot(range(len(betas)),np.array(beta_overall).mean(axis=0),marker='.',color='red')\n",
    "\n",
    "# ax[1].plot(range(len(SWDs)),np.array(SWDs_overall).mean(axis=0),marker='.',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.07042975  0.9495182   0.62380174  1.06918083  1.58137499 -0.05153057\n",
      "  2.95241103  2.8860449   0.96727365  3.33933427]\n"
     ]
    }
   ],
   "source": [
    "###  generate_train_data\n",
    "N=10000\n",
    "drifted_ind, severity, X1,X2 = generateData(d=20,N=N,seed=404,ndrift=10)\n",
    "\n",
    "X1_train_val = X1.copy()\n",
    "X1_train_val_label = np.ones(len(X1_train_val))\n",
    "\n",
    "X2_train_val = X2.copy()\n",
    "X2_train_val_label = np.zeros(len(X2_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.96747666 1.42143353 1.56826515]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def generateData(d, N, seed, ndrift):\n",
    "    np.random.seed(seed)\n",
    "    mu1 = np.random.randn(d)\n",
    "    ind = list(np.random.choice(np.arange(0, d), ndrift, replace=False))\n",
    "    severity = np.random.normal(2, 1, ndrift)\n",
    "    print(severity)\n",
    "    mu2 = mu1.copy()\n",
    "    mu2[ind] = mu2[ind] + severity\n",
    "\n",
    "    Sigma = np.eye(d)\n",
    "    Sigma_y = Sigma.copy()\n",
    "    #Sigma_y[0, 0] = 1\n",
    "    X = np.random.multivariate_normal(mu1, Sigma, size=N)\n",
    "    Y = np.random.multivariate_normal(mu2, Sigma_y, size=N)\n",
    "    return ind, severity, X, Y\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, d, N, seed, ndrift):\n",
    "        # Generate data\n",
    "        self.ind, self.severity, self.X, self.Y = generateData(d, N, seed, ndrift)\n",
    "        # Convert data to torch tensors\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.float32)\n",
    "        \n",
    "        # Labels: 0 for X samples, 1 for Y samples\n",
    "        self.labels_X = torch.zeros(len(self.X), dtype=torch.long)\n",
    "        self.labels_Y = torch.ones(len(self.Y), dtype=torch.long)\n",
    "        \n",
    "        # Combine X and Y with their respective labels\n",
    "        self.data = torch.cat((self.X, self.Y), dim=0)\n",
    "        self.labels = torch.cat((self.labels_X, self.labels_Y), dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of samples (sum of X and Y samples)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a sample and its label as a tuple\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "d = 10   # Number of dimensions\n",
    "N = 5000       # Number of samples per class\n",
    "seed = 44     # Random seed\n",
    "ndrift = 3    # Number of drift dimensions\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = SyntheticDataset(d, N, seed, ndrift)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size - int(0.1 * len(dataset))\n",
    "test_size = int(0.1*len(dataset))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset=dataset, lengths=[train_size, val_size,test_size])\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # Iterate over DataLoader\n",
    "# for data_batch, label_batch in dataloader:\n",
    "#     print(\"Data batch:\", data_batch)\n",
    "#     print(\"Label batch:\", label_batch)\n",
    "#     break  # Just display one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_dim, 64)    # First fully connected layer\n",
    "        # self.fc2 = nn.Linear(64, 32)           # Second fully connected layer\n",
    "        # self.fc3 = nn.Linear(32, 1)            # Output layer for binary classification\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Increased to 128 units\n",
    "        self.fc2 = nn.Linear(128, 64)         # Increased to 64 units\n",
    "        self.fc3 = nn.Linear(64, 32)          # Additional hidden layer\n",
    "        self.fc4 = nn.Linear(32, 1)  \n",
    "        self.sigmoid = nn.Sigmoid()            # Sigmoid for binary output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))          # Sigmoid activation for the output\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        torch.manual_seed(epoch)\n",
    "        for data_batch, label_batch in dataloader:\n",
    "            # Move inputs and labels to device if GPU is used\n",
    "            label_batch = label_batch.float().unsqueeze(1)  # Reshape for BCELoss\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for display\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for this epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:13,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.24806| Train acc:  0.89638| Val loss:  0.21963| Val acc:  0.90873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:04<00:16,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1| Train loss:  0.19895| Train acc:  0.92000| Val loss:  0.18642| Val acc:  0.92361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:07<00:19,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2| Train loss:  0.19917| Train acc:  0.92038| Val loss:  0.20021| Val acc:  0.91667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:11<00:19,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3| Train loss:  0.18859| Train acc:  0.92450| Val loss:  0.19959| Val acc:  0.91964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:15<00:17,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4| Train loss:  0.18964| Train acc:  0.92350| Val loss:  0.18658| Val acc:  0.92460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:19<00:14,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5| Train loss:  0.18363| Train acc:  0.92813| Val loss:  0.18183| Val acc:  0.92361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:23<00:11,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6| Train loss:  0.18329| Train acc:  0.92738| Val loss:  0.18066| Val acc:  0.92361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:26<00:07,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7| Train loss:  0.18261| Train acc:  0.92538| Val loss:  0.18541| Val acc:  0.92758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:30<00:03,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8| Train loss:  0.17952| Train acc:  0.92513| Val loss:  0.18366| Val acc:  0.92460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9| Train loss:  0.17898| Train acc:  0.92800| Val loss:  0.17991| Val acc:  0.93155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "input_dim = d   # Number of features from the dataset\n",
    "model_NN1 = ClassificationNet(input_dim)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "accuracy = BinaryAccuracy()\n",
    "#optimizer = optim.SGD(model_NN1.parameters(), lr=0.001,momentum=0.9)\n",
    "optimizer = optim.Adam(model_NN1.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Experiment tracking\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "experiment_name = \"Synthetic\"\n",
    "model_name = \"NN1\"\n",
    "log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# device-agnostic setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy = accuracy.to(device)\n",
    "model_NN1 = model_NN1.to(device)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    torch.manual_seed(epoch)\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.float().to(device)\n",
    "        \n",
    "        model_NN1.train()\n",
    "        \n",
    "        y_pred = model_NN1(X)\n",
    "        loss = loss_fn(y_pred, y.unsqueeze(1))\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy(y_pred, y.unsqueeze(1))\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model_NN1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dataloader:\n",
    "            X, y = X.to(device), y.float().to(device)\n",
    "        \n",
    "            y_pred = model_NN1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y.unsqueeze(1))\n",
    "            val_acc += acc\n",
    "            \n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        \n",
    "    writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict={\"train/loss\": train_loss, \"val/loss\": val_loss}, global_step=epoch)\n",
    "    writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict={\"train/acc\": train_acc, \"val/acc\": val_acc}, global_step=epoch)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight torch.Size([128, 10])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([64, 128])\n",
      "fc2.bias torch.Size([64])\n",
      "fc3.weight torch.Size([32, 64])\n",
      "fc3.bias torch.Size([32])\n",
      "fc4.weight torch.Size([1, 32])\n",
      "fc4.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_NN1.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.22066| Test acc:  0.91369\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loss_fn = nn.BCELoss().to(device)\n",
    "accuracy = BinaryAccuracy().to(device)\n",
    "\n",
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "#model_lenet5_v1_mnist_loaded.to(device)\n",
    "model_NN1.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.float().to(device)\n",
    "        y_pred = model_NN1(X)\n",
    "        \n",
    "        test_loss += loss_fn(y_pred, y.unsqueeze(1))\n",
    "        test_acc += accuracy(y_pred, y.unsqueeze(1))\n",
    "        \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "print(f\"Test loss: {test_loss: .5f}| Test acc: {test_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9657e-01],\n",
       "        [9.9636e-01],\n",
       "        [9.9992e-01],\n",
       "        [9.9718e-01],\n",
       "        [9.8691e-01],\n",
       "        [9.9902e-01],\n",
       "        [1.9469e-05],\n",
       "        [8.3124e-01],\n",
       "        [9.9939e-01],\n",
       "        [2.5167e-02],\n",
       "        [7.4609e-02],\n",
       "        [9.9961e-01],\n",
       "        [9.9497e-01],\n",
       "        [9.6825e-01],\n",
       "        [9.9999e-01],\n",
       "        [9.9786e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationNet(\n",
       "  (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]c:\\Users\\Sven Jacob\\miniconda3\\envs\\torch\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "100%|██████████| 63/63 [00:14<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients, GradientShap, KernelShap, DeepLift\n",
    "from tqdm import tqdm\n",
    "ig = IntegratedGradients(model_NN1)\n",
    "gs = GradientShap(model_NN1)\n",
    "ks = KernelShap(model_NN1)\n",
    "dl = DeepLift(model_NN1)\n",
    "#sample ,label = test_dataset[10]\n",
    "\n",
    "#tensor_1 = torch.mean(torch.stack(data_c1),axis=0)\n",
    "#tensor_2 = torch.mean(torch.stack(data_c2),axis=0)\n",
    "\n",
    "all_attributions_ig = []\n",
    "all_labels = []\n",
    "all_attributions_gs = []\n",
    "all_attributions_ks = []\n",
    "all_attributions_dl = []\n",
    "model_NN1.eval()\n",
    "for batch_samples, batch_labels in tqdm(test_dataloader):\n",
    "    # Ensure samples have the correct shape\n",
    "    batch_samples = batch_samples.requires_grad_().to(device) # Enable gradients for attribution\n",
    "    batch_labels = batch_labels.to(device)\n",
    "    baseline_dist = torch.zeros((batch_samples.shape[0],input_dim)).to(device)\n",
    "    #baseline_dist = torch.abs(torch.tensor(Contributions[0],dtype=torch.float32).repeat(batch_samples.shape[0],1))\n",
    "\n",
    "    # Calculate the attributions for each sample in the batch\n",
    "    # We use target=0 as we are working with a binary classification output\n",
    "    attributions, deltas = ig.attribute(batch_samples, target=0, return_convergence_delta=True,n_steps=200)\n",
    "\n",
    "    \n",
    "    # Append attributions and labels for further analysis\n",
    "    all_attributions_ig.append(attributions)\n",
    "    all_labels.append(batch_labels)\n",
    "    #batch_samples = batch_samples.requires_grad_()\n",
    "    attributions_gs, deltas = gs.attribute(batch_samples, target=0,  baselines=baseline_dist,return_convergence_delta=True,n_samples=50)\n",
    "    all_attributions_gs.append(attributions_gs)\n",
    "    \n",
    "    #batch_samples = batch_samples.requires_grad_()\n",
    "    attributions_dl = dl.attribute(batch_samples, target=0)\n",
    "    all_attributions_dl.append(attributions_dl)\n",
    "\n",
    "\n",
    "# Concatenate all attributions and labels\n",
    "all_attributions_ig = torch.cat(all_attributions_ig, dim=0)  # Shape: [num_samples, num_features]\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "all_attributions_gs = torch.cat(all_attributions_gs, dim=0)\n",
    "all_attributions_dl = torch.cat(all_attributions_dl, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 2]\n",
      "SWD: 0.7758893377851417\n",
      "[[-0.06045063 -0.00334135 -0.37877322 -0.0252355  -0.00994459  0.01574034\n",
      "   0.02622625 -0.01822402 -0.45519077 -0.33054692]\n",
      " [ 0.03244917  0.00890726  0.37853987  0.0026501   0.02716316 -0.00248297\n",
      "  -0.05640111 -0.00932701  0.4719679   0.30743846]]\n",
      "[-0.06045063 -0.00334135 -0.37877322 -0.0252355  -0.00994459  0.01574034\n",
      "  0.02622625 -0.01822402 -0.45519077 -0.33054692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sven Jacob\\miniconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWD: 0.7823055426974159\n",
      "[[-0.03913338 -0.015826   -0.38786341 -0.03915847  0.003701   -0.00643609\n",
      "   0.00265566  0.02507998 -0.4541438  -0.32511579]\n",
      " [ 0.04383045  0.01551624  0.40096359  0.00099403  0.00366636 -0.01107836\n",
      "   0.02202865 -0.02509701  0.44508309  0.326198  ]]\n",
      "[-0.03913338 -0.015826   -0.38786341 -0.03915847  0.003701   -0.00643609\n",
      "  0.00265566  0.02507998 -0.4541438  -0.32511579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sven Jacob\\miniconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWD: 0.815895889757475\n",
      "[[ 0.02827432  0.0104789   0.38918536  0.03343089  0.01926474 -0.01030321\n",
      "   0.02101738 -0.01237385  0.45606059  0.32223044]\n",
      " [-0.04711919 -0.02844562 -0.39461999 -0.03475534 -0.00053601  0.00367572\n",
      "  -0.00202769 -0.00283137 -0.45421093 -0.34767968]]\n",
      "[ 0.02827432  0.0104789   0.38918536  0.03343089  0.01926474 -0.01030321\n",
      "  0.02101738 -0.01237385  0.45606059  0.32223044]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sven Jacob\\miniconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "\n",
    "Syn_samples = []\n",
    "\n",
    "all_labels = []\n",
    "for batch_samples, batch_labels in test_dataloader:\n",
    "    Syn_samples.append(batch_samples)\n",
    "    all_labels.append(batch_labels)\n",
    "\n",
    "\n",
    "Syn_samples = torch.cat(Syn_samples, dim=0)\n",
    "all_labels = torch.cat(all_labels,dim=0)\n",
    "\n",
    "Syn_X = Syn_samples[all_labels==0].numpy()\n",
    "Syn_Y = Syn_samples[all_labels==1].numpy()\n",
    "\n",
    "print(dataset.ind)\n",
    "rf, betas, SWDs , Contributions = remove_important_features_syn(Syn_X,Syn_Y,3,10000,max_parameter=False,q=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "contributions_plot = np.abs((Contributions)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = tuple([str(i) for i in range(d)])\n",
    "ig_plot = torch.abs(all_attributions_ig[all_labels ==0 ].mean(axis=0)-all_attributions_ig[all_labels ==1 ].mean(axis=0)).detach().cpu().numpy()\n",
    "gs_plot = torch.abs(all_attributions_gs[all_labels ==0 ].mean(axis=0)-all_attributions_gs[all_labels ==1 ].mean(axis=0)).detach().cpu().numpy()\n",
    "dl_plot = torch.abs(all_attributions_dl[all_labels ==0 ].mean(axis=0)-all_attributions_dl[all_labels ==1 ].mean(axis=0)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01783957, -0.03639256, -0.37608461, -0.01454402,  0.00594951,\n",
       "        -0.0098662 ,  0.01113785,  0.01084753, -0.46738715, -0.32929304]),\n",
       " array([ 0.00181259,  0.00520202,  0.36193348,  0.01002458, -0.00800981,\n",
       "         0.02463911, -0.00638874,  0.01784618,  0.48197824,  0.32655414]),\n",
       " array([ 0.00627071, -0.00314288, -0.50346801,  0.00923644,  0.02881554,\n",
       "         0.00493313, -0.04081449, -0.03270971,  0.01488048, -0.48107194])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "data = {\n",
    "    'Features': list(bars),\n",
    "    'IG': ig_plot,\n",
    "    'GS': gs_plot,\n",
    "    'DL': dl_plot,\n",
    "    'swd_group': np.abs(Contributions[0]) # This will be plotted on the secondary y-axis\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.set_index('Features', inplace=True)\n",
    "\n",
    "# Plot settings\n",
    "bar_width = 0.2  # Width of each bar\n",
    "x = np.arange(len(df))  # X positions for the features\n",
    "plt.rcParams['hatch.linewidth'] = 1  # Default is 1.0\n",
    "#hatch_patterns = ['..', '..', '..']  # Hatches for the first 3 groups\n",
    "hatch_patterns = ['', '', '']\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot the first three groups with hatches on the primary y-axis\n",
    "colors = [plt.colormaps.get_cmap('tab20c')(18),plt.colormaps.get_cmap('tab20c')(17),plt.colormaps.get_cmap('tab20c')(16)]\n",
    "for i, (group, hatch) in enumerate(zip(df.columns[:-1], hatch_patterns)):  # Exclude 'Group 4'\n",
    "    ax.bar(x + i * bar_width, df[group], bar_width, label=group, hatch=hatch,color=colors[i],edgecolor='black',lw=2,alpha=1)\n",
    "\n",
    "# Create a secondary y-axis for 'Group 4'\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x + 3 * bar_width, df['swd_group'], bar_width, label='SWD', color='blue',edgecolor='black',lw=2,alpha=0.9)\n",
    "\n",
    "# Customizations for primary y-axis\n",
    "#ax.set_ylabel('Values (Groups 1-3)')\n",
    "ax.set_xticks(x + bar_width * 1.5)  # Align x-axis labels to the center of the groups\n",
    "ax.set_xticklabels(df.index)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "for i in dataset.ind:\n",
    "    ax.xaxis.get_ticklabels()[i].set_bbox(dict(facecolor='none',edgecolor='red'))\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n",
    "# Customizations for secondary y-axis\n",
    "#ax2.set_ylabel(color='lightcoral')\n",
    "ax2.legend(loc='upper right', frameon=True)\n",
    "\n",
    "# Aligning grid and layout\n",
    "plt.title('Explanation scores')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('C:/Users/Sven Jacob/Downloads/Syn_Experiment1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8686595600041314"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = lambda a,b : np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "cos_sim(data['swd_group'],dl_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m     19\u001b[0m     REPORT[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m---> 20\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#target = torch.abs(y-1)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     x_fgm \u001b[38;5;241m=\u001b[39m fast_gradient_method(model_NN1,x,eps,norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "\n",
    "\n",
    "#model_loaded = model_NN1\n",
    "\n",
    "model_NN1.eval()\n",
    "epsilons = [1]\n",
    "accuracies = []\n",
    "#eps = 0.0005\n",
    "REPORTS= []\n",
    "for eps in epsilons:\n",
    "    nb_missclassified_FGSM=0\n",
    "    nb_missclassified_PGD=0\n",
    "\n",
    "    REPORT = {'original':{'y':[],'x':[]},'attack_FGSM':{'y':[],'x':[]},'attack_PGD':{'y':[],'x':[]}}\n",
    "    #torch.manual_seed(10)\n",
    "    for x,y in test_dataloader:\n",
    "        REPORT['original']['x'].append(x)\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        #target = torch.abs(y-1)\n",
    "        x_fgm = fast_gradient_method(model_NN1,x,eps,norm=2)\n",
    "        REPORT['attack_FGSM']['x'].append(x_fgm.detach().cpu())\n",
    "\n",
    "       # x_pgd = projected_gradient_descent(model_loaded,x,eps, eps_iter=0.0001, nb_iter=40, norm=2)\n",
    "        #REPORT['attack_PGD']['x'].append(x_pgd.detach().cpu())\n",
    "\n",
    "\n",
    "        _, y_pred = model_NN1(x).max(1)\n",
    "        REPORT['original']['y'].append(y.detach().cpu())\n",
    "\n",
    "        _, y_pred_fgm = model_NN1(x_fgm).max(1)\n",
    "        REPORT['attack_FGSM']['y'].append(y_pred_fgm.detach().cpu())\n",
    "\n",
    "        #_, y_pred_pgd = model_loaded(x_pgd).max(1)\n",
    "        #REPORT['attack_PGD']['y'].append(y_pred_pgd.detach().cpu())\n",
    "\n",
    "     \n",
    "\n",
    "        nb_missclassified_FGSM+= torch.sum((y_pred_fgm == y_pred))\n",
    "        #nb_missclassified_PGD+= torch.sum((y_pred_pgd == y_pred))\n",
    "\n",
    "    REPORT['original']['y'] = torch.cat(REPORT['original']['y'],axis=0)\n",
    "    REPORT['original']['x'] = torch.cat(REPORT['original']['x'],axis=0)\n",
    "\n",
    "    REPORT['attack_FGSM']['y'] = torch.cat(REPORT['attack_FGSM']['y'],axis=0)\n",
    "    REPORT['attack_FGSM']['x'] = torch.cat(REPORT['attack_FGSM']['x'],axis=0)\n",
    "\n",
    "    #REPORT['attack_PGD']['y'] = torch.cat(REPORT['attack_PGD']['y'],axis=0)\n",
    "    #REPORT['attack_PGD']['x'] = torch.cat(REPORT['attack_PGD']['x'],axis=0)\n",
    "    acc_FGSM = nb_missclassified_FGSM.detach().cpu().numpy()/len(test_dataset)\n",
    "    #acc_PGD = nb_missclassified_PGD.detach().cpu().numpy()/len(test_dataset)\n",
    "\n",
    "    accuracies.append(acc_FGSM)\n",
    "    print(eps,acc_FGSM)\n",
    "    #print(eps, acc_PGD)\n",
    "\n",
    "    REPORTS.append(REPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FGSM attack function\n",
    "def fgsm_attack(model, criterion, inputs, labels, epsilon):\n",
    "    \"\"\"\n",
    "    Performs FGSM attack on the input samples.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        criterion: Loss function used for training\n",
    "        inputs: Original input data (torch.Tensor)\n",
    "        labels: True labels corresponding to the inputs\n",
    "        epsilon: Perturbation magnitude (float)\n",
    "    \n",
    "    Returns:\n",
    "        perturbed_inputs: Adversarial examples created by FGSM\n",
    "    \"\"\"\n",
    "    # Ensure input requires gradient for FGSM\n",
    "    inputs.requires_grad = True\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Compute the sign of the gradients\n",
    "    data_grad = inputs.grad.data\n",
    "    sign_data_grad = data_grad.sign()\n",
    "\n",
    "    # Create perturbed input\n",
    "    perturbed_inputs = inputs + epsilon * sign_data_grad\n",
    "    perturbed_inputs = torch.clamp(perturbed_inputs, 0, 1)  # Clamp to valid range (optional)\n",
    "\n",
    "    return perturbed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 10\tTest Accuracy = 47.30%\n"
     ]
    }
   ],
   "source": [
    "epsilon = 10  # Choose an epsilon value\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "model_NN1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "adv_examples = []\n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Convert target to match the output size for BCE Loss\n",
    "    target = target.view(-1, 1).float()\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    perturbed_data = fgsm_attack(model_NN1, criterion, data, target, epsilon)\n",
    "\n",
    "    # Re-evaluate the model with perturbed data\n",
    "    output = model_NN1(perturbed_data)\n",
    "    pred = (output > 0.5).float()  # Binary prediction\n",
    "\n",
    "    # Track accuracy\n",
    "    correct += (pred == target).sum().item()\n",
    "    \n",
    "\n",
    "    # Optionally, save adversarial examples\n",
    "    for i in range(data.size(0)):  # Iterate over batch\n",
    "        if pred[i] != target[i]:  # Check if attack was successful\n",
    "            adv_examples.append((data[i].detach().cpu(), perturbed_data[i].detach().cpu()))\n",
    "    \n",
    "\n",
    "# Calculate final accuracy\n",
    "final_acc = correct / len(test_dataloader.dataset)\n",
    "print(f\"Epsilon: {epsilon}\\tTest Accuracy = {final_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.zeros(10).to(device)\n",
    "\n",
    "for i in range(len(adv_examples)):\n",
    "    l+=torch.abs(adv_examples[i][0]-adv_examples[i][1]).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9353,  0.8520,  2.5920, -1.1417, -1.3757, -2.5052,  1.6496, -0.0100,\n",
       "         1.1052,  0.3070])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_examples[0][0]-adv_examples[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
